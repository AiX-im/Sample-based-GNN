### push down流水线设计
1. 有一个CPU线程一直在对所有节点进行第一层的全聚合，同时设计一个bitmap来记录哪些顶点完成了聚合
2. 如果完成了所有节点的第一层全聚合，则该线程会停止（或者睡眠，即作为全局线程？），如果weight被改变了，该线程也会停止（一个atomic标志）
3. 然后流水线那里和SampleGPU, SampleAllGPU一样，即依次进行采样（要少采一层样）、传输embedding、GPU训练这些

#### 问题
1. 第一层的W需要存储在CPU上，第一层聚合feature的tensor也需要存在CPU上，所以反向的梯度也需要传回来（或者保存在GPU中到一定程度才传回来？）
2. 第一层的聚合无法使用稀疏矩阵乘法(稀疏矩阵乘法是GPU的)，第一层聚合的结果也无法进行复用（因为是乘W后的），或者可以增加一个配置项，保存乘W前的
3. 第一层聚合和反向不可以添加到算子栈中（因为算子栈会自动进行反向），需要手动调用反向

#### 验证
1. 从tensor中可以直接获得上一层传过来的梯度
